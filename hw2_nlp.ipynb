{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw 2 NLP",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zB2BpE6DhW"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "This assignment is about training and evaluating a POS tagger with some real data. The dataset is available through the Universal Dependencies (https://universaldependencies.org/) (UD) project. To get to know the project, please visit https://universaldependencies.org/introduction.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU8Qgi-eLoWL",
        "outputId": "7fa727ad-1bd2-47df-9012-978ecb534a30"
      },
      "source": [
        "!pip install conllutils\n",
        "!pip install conll_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conllutils\n",
            "  Downloading https://files.pythonhosted.org/packages/20/33/508eabad4f84f65971f3eca651478e4c378db1f44070d76a20c0eacf347d/conllutils-1.1.4.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from conllutils) (1.19.5)\n",
            "Building wheels for collected packages: conllutils\n",
            "  Building wheel for conllutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for conllutils: filename=conllutils-1.1.4-cp37-none-any.whl size=17696 sha256=c3b96ebaa6b11c57f3954a2d5c39bd5c4ab73c8e574fc580f5881f0390d804bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/08/3a/74fab1fdde78b548f26b2de930af3cee81a592fafe9e758a57\n",
            "Successfully built conllutils\n",
            "Installing collected packages: conllutils\n",
            "Successfully installed conllutils-1.1.4\n",
            "Collecting conll_df\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/fe/a562090ef7651a00f2696aa384279724ed1f6f90f1b2b5e7a98fb9bb151f/conll_df-0.0.4.tar.gz\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from conll_df) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->conll_df) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->conll_df) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->conll_df) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19.2->conll_df) (1.15.0)\n",
            "Building wheels for collected packages: conll-df\n",
            "  Building wheel for conll-df (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for conll-df\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for conll-df\n",
            "Failed to build conll-df\n",
            "Installing collected packages: conll-df\n",
            "    Running setup.py install for conll-df ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed conll-df-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRm7zcfq56HF"
      },
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import conllutils\n",
        "import random\n",
        "from collections import Counter\n",
        "from conll_df import conll_df\n",
        "from nltk.tag import tnt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-Xvqip6Teu"
      },
      "source": [
        "**Part 1** (getting the data)\n",
        "\n",
        "You can download the dataset files directly from the UD website, but it will let you only download all the languages in one compressed file. In this assignment you will be working with th GUM dataset, which you can download directly from:\n",
        "https://github.com/UniversalDependencies/UD_English-GUM.\n",
        "Please download it to your colab machine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsZsyTVC6Sw0",
        "outputId": "b41b799b-2f76-4f2c-96d2-7644aacfe7f6"
      },
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_English-GUM\n",
        "%cd /content/UD_English-GUM/\n",
        "!git checkout 2c8b062269f2d2d3d62405c82d8c25cf24f705dd\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UD_English-GUM'...\n",
            "remote: Enumerating objects: 2526, done.\u001b[K\n",
            "remote: Counting objects: 100% (1537/1537), done.\u001b[K\n",
            "remote: Compressing objects: 100% (667/667), done.\u001b[K\n",
            "remote: Total 2526 (delta 1347), reused 1055 (delta 870), pack-reused 989\u001b[K\n",
            "Receiving objects: 100% (2526/2526), 16.21 MiB | 9.40 MiB/s, done.\n",
            "Resolving deltas: 100% (2237/2237), done.\n",
            "/content/UD_English-GUM\n",
            "Note: checking out '2c8b062269f2d2d3d62405c82d8c25cf24f705dd'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 2c8b062 Updated statistics.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZGOtoteWHz"
      },
      "source": [
        "We will use the (train/dev/test) files:\n",
        "\n",
        "UD_English-GUM/en_gum-ud-train.conllu\n",
        "\n",
        "UD_English-GUM/en_gum-ud-dev.conllu\n",
        "\n",
        "UD_English-GUM/en_gum-ud-test.conllu\n",
        "\n",
        "They are all formatted in the conllu format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.\n",
        "\n",
        "You should write a code that reads the three datasets into memory. You may choose the data structure by yourself. As you can see, every word is represented by a line, with columns representing specific features. We are only interested in the first and fourth columns, corresponding to the word and its POS tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "v7A0-DjWg2JW",
        "outputId": "28511bbe-bc4f-4f7b-d6e3-30361ce1a102"
      },
      "source": [
        "# Your code goes here\n",
        "train_path = \"UD_English-GUM/en_gum-ud-train.conllu\"\n",
        "#train_path = 'previous/en_gum-ud-train.conllu'\n",
        "test_path = 'UD_English-GUM/en_gum-ud-test.conllu'\n",
        "dev_path = 'UD_English-GUM/en_gum-ud-dev.conllu'\n",
        "\n",
        "#column x is upos tag\n",
        "file = conll_df(train_path, file_index=False)\n",
        "train = file[['w', 'x']].copy()\n",
        "\n",
        "file = conll_df(test_path, file_index=False)\n",
        "test = file[['w', 'x']].copy()\n",
        "\n",
        "file = conll_df(dev_path, file_index=False)\n",
        "dev = file[['w', 'x']].copy()\n",
        "\n",
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>w</th>\n",
              "      <th>x</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s</th>\n",
              "      <th>i</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>1</th>\n",
              "      <td>Introduction</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
              "      <th>1</th>\n",
              "      <td>Research</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>on</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adult-learned</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>second</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 w     x\n",
              "s i                     \n",
              "1 1   Introduction  NOUN\n",
              "2 1       Research  NOUN\n",
              "  2             on   ADP\n",
              "  3  adult-learned   ADJ\n",
              "  4         second   ADJ"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Z9BMNM7EP3"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a class **simple_tagger**, with methods *train* and *evaluate*. The method *train* receives the data as a list of sentences, and use it for training the tagger. In this case, it should learn a simple dictionary that maps words to tags, defined as the most frequent tag for every word (in case there is more than one most frequent tag, you may select one of them randomly). The dictionary should be stored as a class member for evaluation.\n",
        "\n",
        "The method *evaluate* receives the data as a list of sentences, and use it to evaluate the tagger performance. Specifically, you should calculate the word and sentence level accuracy.\n",
        "The evaluation process is simply going word by word, querying the dictionary (created by the train method) for each word’s tag and compare it to the true tag of that word. The word-level accuracy is the number of successes divided by the number of words. For OOV (out of vocabulary, or unknown) words, the tagger should assign the most frequent tag in the entire training set (i.e., the mode). The function should return the two numbers: word level accuracy and sentence level accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtivZLBH7dXq"
      },
      "source": [
        "class simple_tagger:\n",
        "  def __init__(self):\n",
        "    self.tagger = {}\n",
        "    self.most_freq_tag = ''\n",
        "\n",
        "  def train(self, data):\n",
        "    train_df = data.copy()\n",
        "    train_df['count'] = 1\n",
        "\n",
        "    new = train_df.groupby(['w','x'])['count'].count().reset_index()\n",
        "    new.dropna(axis = 'rows', inplace = True)\n",
        "\n",
        "    df = new.groupby(['w'])['count'].max().reset_index()\n",
        "    df = df.merge(new, how='left', on=['w','count']) \n",
        "    df = df[['w','x']]\n",
        "    d = df.set_index('w')['x'].to_dict()\n",
        "\n",
        "    self.tagger = d\n",
        "    self.most_freq_tag = Counter(d.values()).most_common(1)[0][0]\n",
        "    return d\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    num_sent = data.groupby('s').count().shape[0]\n",
        "    data['counter'] = 0\n",
        "    \n",
        "    def compare_tags(row):\n",
        "      word = row[0]\n",
        "      t = row[1]\n",
        "      d = self.tagger\n",
        "      if word in d:\n",
        "        if t == d[word]:\n",
        "          row['counter'] = 1\n",
        "      else:\n",
        "        if t == self.most_freq_tag:\n",
        "          row['counter'] = 1\n",
        "      return row['counter'] \n",
        "\n",
        "    # Word accuracy\n",
        "    data['counter'] = data.apply(compare_tags, axis = 1)\n",
        "    word_count = sum(data['counter'])\n",
        "\n",
        "    # Sentence accuracy\n",
        "    df =  data['counter'].groupby(level=[0]).count() - data['counter'].groupby(level=[0]).sum() \n",
        "    # If df = 0 then all words in sentence have correct tag\n",
        "    sent_count = df[df == 0].count()\n",
        "\n",
        "    w_accuracy = word_count/data.shape[0]\n",
        "    s_accurary = sent_count/num_sent\n",
        "    return w_accuracy, s_accurary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_WsokaDLiv2",
        "outputId": "765feada-9631-47cd-a24f-9f7495f87121"
      },
      "source": [
        "tag = simple_tagger()\n",
        "tag.train(train)\n",
        "wtest, stest = tag.evaluate(test)\n",
        "wdev, sdev = tag.evaluate(dev)\n",
        "\n",
        "#For Part 4\n",
        "save_results_simple_test = ['Simple Tagger', wtest, stest]\n",
        "save_results_simple_dev = ['Simple Tagger', wdev, sdev]\n",
        "\n",
        "print(f'Word accuracy on test data: {wtest} \\nSentence accuracy on test data: {stest}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word accuracy on test data: 0.8460379254049981 \n",
            "Sentence accuracy on test data: 0.1898876404494382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etK9iZIq8i0X"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Similar to part 2, write the class hmm_tagger, which implements HMM tagging. The method *train* should build the matrices A, B and Pi, from the data as discussed in class. The method *evaluate* should find the best tag sequence for every input sentence using he Viterbi decoding algorithm, and then calculate the word and sentence level accuracy using the gold-standard tags. You should implement the Viterbi algorithm in the next block and call it from your class.\n",
        "\n",
        "Additional guidance:\n",
        "1. The matrix B represents the emissions probabilities. Since B is a matrix, you should build a dictionary that maps every unique word in the corpus to a serial numeric id (starting with 0). This way columns in B represents word ids.\n",
        "2. During the evaluation, you should first convert each word into it’s index and then create the observation array to be given to Viterbi, as a list of ids. OOV words should be assigned with a random tag. To make sure Viterbi works appropriately, you can simply break the sentence into multiple segments every time you see an OOV word, and decode every segment individually using Viterbi.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpH7GuiQ9L6W"
      },
      "source": [
        "class hmm_tagger:\n",
        "  def __init__(self):\n",
        "    self.A = np.array([])\n",
        "    self.B = np.array([])\n",
        "    self.Pi = np.array([])\n",
        "    self.words = {}\n",
        "\n",
        "  def train(self, data):\n",
        "    unique_tags = list(data['x'].unique())\n",
        "\n",
        "    # Matrix A\n",
        "    num_sent = data.groupby('s').count().shape[0]\n",
        "    \n",
        "    tr = data.loc[1]\n",
        "    df = pd.DataFrame()\n",
        "    df['a'] = tr['x'][0:-1]\n",
        "    df['b'] =  tr['x'].shift(-1)\n",
        "    df['count'] = int(1)\n",
        "\n",
        "    for j in range(2, num_sent + 1): \n",
        "      tr = data.loc[j]\n",
        "      if tr.shape[0] > 1:\n",
        "        df3 = pd.DataFrame()\n",
        "        df3['a'] = tr['x'][0:-1]\n",
        "        df3['b'] =  tr['x'].shift(-1)\n",
        "        df3['count'] = int(1)\n",
        "        df = pd.concat([df, df3], axis=0)\n",
        "\n",
        "    tag_probability = df.groupby(['a','b']).count()\n",
        "    tag_probability.sort_index(inplace = True)\n",
        "    new = pd.DataFrame(tag_probability)\n",
        "    new.reset_index(inplace = True)\n",
        "    pivoted = new.pivot(index=\"a\", columns=\"b\")\n",
        "    pivoted = pivoted.fillna(0)\n",
        "    s = pivoted.sum(axis = 1)\n",
        "\n",
        "    for i in range(pivoted.shape[0]):\n",
        "      pivoted.iloc[i,:] = pivoted.iloc[i,:]/s[i]\n",
        "\n",
        "    self.A = pivoted\n",
        "\n",
        "    # Matrix B\n",
        "    new_df = data\n",
        "    new_df['count'] = 1\n",
        "    saved = new_df.groupby(['x','w']).count()\n",
        "    saved.sort_index(inplace = True)\n",
        "    saved.columns\n",
        "    new = pd.DataFrame(saved)\n",
        "\n",
        "    new.reset_index(inplace = True)\n",
        "    pivoted = new.pivot(index=\"x\", columns=\"w\")\n",
        "    pivoted = pivoted.fillna(0)\n",
        "    s = pivoted.sum(axis = 1)\n",
        "\n",
        "    for i in range(pivoted.shape[0]):\n",
        "      pivoted.iloc[i,:] = pivoted.iloc[i,:]/s[i]\n",
        "    \n",
        "    self.B = pivoted\n",
        "\n",
        "    # Pi Matrix\n",
        "    self.Pi = [1/len(unique_tags) for i in unique_tags]\n",
        "    self.Pi = np.array(self.Pi)\n",
        "\n",
        "    # Unique words of train set\n",
        "    words = []\n",
        "    for i in pivoted.columns:\n",
        "      words.append(i[1])\n",
        "\n",
        "    # Dictionary that maps every unique word in the corpus to a serial numeric id \n",
        "    words_dict = {words[x]:x for x in range(len(words))}\n",
        "    self.words = words_dict\n",
        "\n",
        "    return self.A, self.B, self.Pi\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    unique_works = list(data['w'].unique())\n",
        "    num_sent = data.groupby('s').count().shape[0]\n",
        "    best_sequence = []\n",
        "\n",
        "    test_data = data.copy()\n",
        "    df1 = pd.DataFrame()\n",
        "    curr_row = 0\n",
        "\n",
        "    for i in range(1,num_sent+1):\n",
        "      df2 = test_data.iloc[curr_row:test_data.loc[(i)].shape[0]+curr_row,:]\n",
        "      curr_row += test_data.loc[(i)].shape[0]\n",
        "      sentence = list(test_data.loc[(i)]['w'])\n",
        "      observation = []\n",
        "      for word in sentence:\n",
        "        if word in self.words:\n",
        "           observation.append(self.words[word])\n",
        "        else:\n",
        "            r = random.randint(0,len(self.words))  \n",
        "            observation.append(r)\n",
        "\n",
        "      best_sequence = viterbi(np.array(observation), self.A.to_numpy(), self.B.to_numpy(), self.Pi)\n",
        "      \n",
        "      result = []\n",
        "      for i in best_sequence:\n",
        "        result.append(self.A.columns[i][1])\n",
        "      \n",
        "      df2.insert(2, 'tag', result)\n",
        "      df1 = pd.concat([df1, df2])\n",
        "\n",
        "    df1['counter'] = 0\n",
        "\n",
        "    def compare_tags(row):\n",
        "      if row['x'] == row['tag']:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "    # Word accuracy\n",
        "    df1['counter'] = df1.apply(compare_tags, axis = 1)\n",
        "    word_count = sum(df1['counter'])\n",
        "\n",
        "    # Sentence accuracy\n",
        "    df =  df1['counter'].groupby(level=[0]).count() - df1['counter'].groupby(level=[0]).sum() \n",
        "    # If df = 0 then all words in sentence have correct tag\n",
        "    sent_count = df[df == 0].count()\n",
        "\n",
        "    w_accuracy = word_count/df1.shape[0]\n",
        "    s_accurary = sent_count/num_sent\n",
        "    return w_accuracy, s_accurary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6KJW2F9yqt"
      },
      "source": [
        "# Viterbi\n",
        "np.seterr(divide = 'ignore')\n",
        "\n",
        "def viterbi (observations, A, B, Pi):\n",
        "  obs = len(observations)\n",
        "  a = A.shape[0]\n",
        "  v = np.zeros((obs, a))\n",
        "  v[0, :] = np.log(Pi[0] * B[:, observations[0]])\n",
        "  prev = np.zeros((obs - 1, a))\n",
        "  best_sequence = []\n",
        "\n",
        "  for i in range(1, obs):\n",
        "    for j in range(a):\n",
        "        prob = v[i - 1] + np.log(A[:, j]) + np.log(B[j, observations[i]])\n",
        "        prev[i - 1, j] = np.argmax(prob)\n",
        "        v[i, j] = np.max(prob)\n",
        "\n",
        "  v_result = np.zeros(obs)\n",
        "  last_state = np.argmax(v[obs - 1, :])\n",
        "  v_result[0] = last_state\n",
        "\n",
        "  backtrack_index = 1\n",
        "  for i in range(obs - 2, -1, -1):\n",
        "      v_result[backtrack_index] = prev[i, int(last_state)]\n",
        "      last_state = prev[i, int(last_state)]\n",
        "      backtrack_index += 1\n",
        "  best_sequence = np.flip(v_result, axis=0)\n",
        "  best_sequence = [int(s) for s in best_sequence]\n",
        "  return best_sequence\n",
        "\n",
        "# A simple example to run the Viterbi algorithm:\n",
        "#( Same as in presentation \"NLP 3 - Tagging\" on slide 35)\n",
        "# A = np.array([[0.3, 0.7], [0.2, 0.8]])\n",
        "# B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])\n",
        "# Pi = np.array([0.4, 0.6])\n",
        "# print(viterbi([0, 3, 2, 0], A, B, Pi))\n",
        "# Expected output: 1, 1, 1, 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqy3QofS-t2N",
        "outputId": "6a8daea2-f753-4b0c-fd09-290c26e7a046"
      },
      "source": [
        "tagger = hmm_tagger()\n",
        "A, B, Pi = tagger.train(train)\n",
        "hmmw, hmms = tagger.evaluate(test)\n",
        "hmmw2, hmms2 = tagger.evaluate(dev)\n",
        "\n",
        "#For Part 4\n",
        "saved_results_hmm_test = ['HMM Tagger', hmmw, hmms]\n",
        "saved_results_hmm_dev = ['HMM Tagger', hmmw2, hmms2]\n",
        "\n",
        "print(f'Word accuracy on test data: {hmmw} \\nSentence accuracy on test data: {hmms}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word accuracy on test data: 0.8314705512997614 \n",
            "Sentence accuracy on test data: 0.15842696629213482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YZO0uGL-4S-"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Compare the results obtained from both taggers and a MEMM tagger, implemented by NLTK (a known NLP library), over both, the dev and test datasets. To train the NLTK MEMM tagger you should execute the following lines (it may take some time to train...):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYhtboJm_Iyx"
      },
      "source": [
        "def prepare_data(filepath):\n",
        "  output = []\n",
        "  file = conllutils.read_conllu(filepath)\n",
        "  for s in file:\n",
        "    output.append(s)\n",
        "  return output\n",
        "\n",
        "train = prepare_data(train_path)\n",
        "test = prepare_data(test_path)\n",
        "dev = prepare_data(dev_path)\n",
        "\n",
        "def prepare_data_for_eval(data):\n",
        "  return [[(token.lemma, token.upos) for token in sentence] for sentence in data]\n",
        "\n",
        "def sentence_accuracy(data, tagger):\n",
        "  num_sentence = len(data)\n",
        "  sent_counter = 0\n",
        "  for sentence in data:\n",
        "    data = [[(token.lemma, token.upos) for token in sentence]]\n",
        "    result = tagger.evaluate(data)\n",
        "    if result == 1.0:\n",
        "      sent_counter += 1\n",
        "  return sent_counter/num_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAgvQ0BsDrWL"
      },
      "source": [
        "#Load data\n",
        "train_data = prepare_data(train_path)\n",
        "test_data = prepare_data(test_path)\n",
        "dev_data = prepare_data(dev_path)\n",
        "\n",
        "#Initialize tagger\n",
        "tnt_pos_tagger = tnt.TnT()\n",
        "tnt_pos_tagger.train(prepare_data_for_eval(train_data))\n",
        "\n",
        "#Evaluate\n",
        "w_accuracy = tnt_pos_tagger.evaluate(prepare_data_for_eval(test_data))\n",
        "s_accuracy = sentence_accuracy(test_data, tnt_pos_tagger)\n",
        "memm_test = [\"MEMM Tagger\",w_accuracy, s_accuracy ]\n",
        "\n",
        "w_accuracy_d = tnt_pos_tagger.evaluate(prepare_data_for_eval(dev_data))\n",
        "s_accuracy_d = sentence_accuracy(dev_data, tnt_pos_tagger)\n",
        "memm_dev = [\"MEMM Tagger\",w_accuracy_d, s_accuracy_d ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIvvzsq_U-o"
      },
      "source": [
        "Print both, word level and sentence level accuracy for all the three taggers in a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V32202cikh7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b119465-aaa5-409e-f585-1bec8f239a45"
      },
      "source": [
        "# Your code goes here\n",
        "df_test = pd.DataFrame(data = [save_results_simple_test, saved_results_hmm_test, memm_test], columns = ['Tagger Model', 'Word Accuracy', 'Sentence Accuracy'])\n",
        "df_dev = pd.DataFrame(data= [save_results_simple_dev, saved_results_hmm_dev, memm_dev], columns = ['Tagger Model', 'Word Accuracy', 'Sentence Accuracy'])\n",
        "\n",
        "print('Evaluation on test data: \\n')\n",
        "print(df_test.to_string(index=False))\n",
        "print('\\n')\n",
        "print('Evaluation on dev data: \\n')\n",
        "print(df_dev.to_string(index = False))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation on test data: \n",
            "\n",
            "  Tagger Model  Word Accuracy  Sentence Accuracy\n",
            " Simple Tagger       0.846038           0.189888\n",
            "    HMM Tagger       0.831471           0.158427\n",
            "   MEMM Tagger       0.827452           0.153933\n",
            "\n",
            "\n",
            "Evaluation on dev data: \n",
            "\n",
            "  Tagger Model  Word Accuracy  Sentence Accuracy\n",
            " Simple Tagger       0.856777           0.161990\n",
            "    HMM Tagger       0.850814           0.164541\n",
            "   MEMM Tagger       0.851840           0.164541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG1kQqerC6G-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}