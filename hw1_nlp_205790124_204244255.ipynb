{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW_1_205790124_204244255.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyu2kRpQrFk7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re, string, collections\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM"
      },
      "source": [
        "*As a preparation for this task, download the data files from the course git repository.\n",
        "\n",
        "The relevant files are under **lm-languages-data-new**:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "*   test.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xC-87z2GWMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b7ca57-8506-48a2-c803-691bad05f071"
      },
      "source": [
        "!git clone https://github.com/kfirbar/nlp-course.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-course'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 53 (delta 23), reused 32 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOVb4IhsqimJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYdhPfbAGkip",
        "outputId": "31713ff6-7082-44b1-e13c-b1e0b875b7e3"
      },
      "source": [
        "!ls nlp-course/lm-languages-data-new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en.csv\t es.json  in.csv   it.json  pt.csv    test.json   tl.csv\n",
            "en.json  fr.csv   in.json  nl.csv   pt.json   tests.csv   tl.json\n",
            "es.csv\t fr.json  it.csv   nl.json  test.csv  tests.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj"
      },
      "source": [
        "def preprocess():\n",
        "  current_set = set()\n",
        "\n",
        "  for name in glob.glob('nlp-course/lm-languages-data-new/*.csv'):\n",
        "    if 'test' not in name:\n",
        "      id = pd.read_csv(name)\n",
        "            \n",
        "      for i in id.index:\n",
        "        add = set(id.iloc[i,1])\n",
        "        current_set =  current_set | add \n",
        "\n",
        "  vocabulary = list(current_set)\n",
        "  return vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ"
      },
      "source": [
        "def add_start_end(row, n):\n",
        "  row = 'א'*(n-1) + row + 'ת'\n",
        "  return row\n",
        "\n",
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "\n",
        "  df = pd.read_csv(data_file_path)\n",
        "  n_gram_dictionary = collections.Counter()\n",
        "  n_minus_1_gram_dict = collections.Counter()\n",
        "\n",
        "  # Add start and end chars\n",
        "  df['tweet_text'] = df.apply(lambda data: add_start_end(data['tweet_text'], n),axis=1, result_type='expand')  \n",
        "  \n",
        "  # Split the data to n-grams\n",
        "  for i in df.index:\n",
        "    tokens = str(df.iloc[i, 1])\n",
        "    line_length = len(tokens)\n",
        "    if line_length > 0:\n",
        "      manual_partial_ngram = []\n",
        "      manual_partial_ngram_minus_1 = []\n",
        "      for j in range(line_length-n):\n",
        "        nword_split = tuple(list(tokens[j:j+n]))\n",
        "        manual_partial_ngram.append(nword_split)\n",
        "        nword_split_minus_1 = tuple(list(tokens[j:j+n-1]))\n",
        "        manual_partial_ngram_minus_1.append(nword_split_minus_1)\n",
        "\n",
        "    n_gram_dictionary.update(manual_partial_ngram)\n",
        "    n_minus_1_gram_dict.update(manual_partial_ngram_minus_1)\n",
        "\n",
        "  # Calculate the frequency (counts)\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  if n == 1:\n",
        "    model = defaultdict(lambda: 0)\n",
        "    total_count = float(sum(n_gram_dictionary.values()))\n",
        "    for key, value in n_gram_dictionary.items():\n",
        "      model[key] = value/total_count\n",
        "\n",
        "  else:\n",
        "    for key, value in n_gram_dictionary.items():\n",
        "      n_minus_1_gram = tuple(list(key[0:(n-1)]))\n",
        "      nth = key[-1]\n",
        "      model[n_minus_1_gram][nth] = value\n",
        "\n",
        "    # Create frequency distributions (probablity)\n",
        "    for w_n_minus_1 in model:\n",
        "        total_count = float(sum(model[w_n_minus_1].values()))\n",
        "        for w_n in model[w_n_minus_1]:\n",
        "            model[w_n_minus_1][w_n] /= total_count \n",
        "\n",
        "  # Add one smoothing implementation \n",
        "  if add_one:\n",
        "    v = len(vocabulary)\n",
        "    model_smooth = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "    if n == 1:\n",
        "      total_count = float(sum(n_gram_dictionary.values()))\n",
        "      model_smooth = defaultdict(lambda: 1/(total_count+v))\n",
        "      for keys, values in n_gram_dictionary.items():\n",
        "        prob = (values+1)/(total_count+v)  \n",
        "        model_smooth[keys] = prob\n",
        "      \n",
        "    else: \n",
        "      for key in n_minus_1_gram_dict:\n",
        "        C_n_minus_1 = n_minus_1_gram_dict[key]\n",
        "        val = 1/(C_n_minus_1+v) \n",
        "        model_smooth[key] = defaultdict(lambda: val)\n",
        "\n",
        "      for keys, values in n_gram_dictionary.items():\n",
        "        C_n = values \n",
        "        w_n_minus_1 = keys[0:(n-1)]\n",
        "        nth = keys[-1]\n",
        "        C_n_minus_1 = n_minus_1_gram_dict[w_n_minus_1]\n",
        "        prob = (C_n+1)/(C_n_minus_1+v)  \n",
        "        model_smooth[w_n_minus_1][nth] = prob\n",
        "    return model_smooth\n",
        "  \n",
        "  else:\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsaiTIpJADrC"
      },
      "source": [
        "#Run vocab \n",
        "vocab = preprocess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEB61naVBt_k",
        "outputId": "96bf3149-35fa-4c7a-a3cc-8130f48eb07b"
      },
      "source": [
        "#For testing\n",
        "model = lm(3, vocab, 'nlp-course/lm-languages-data-new/es.csv', True)\n",
        "modelF = lm(3, vocab, 'nlp-course/lm-languages-data-new/es.csv', False)\n",
        "\n",
        "# test for unigram\n",
        "# print(model[('R',)]) \n",
        "# print(modelF[('R',)])\n",
        "\n",
        "# # test for bigram\n",
        "# print(model[('R', )]['T']) \n",
        "# print(modelF[('R', )]['T'])\n",
        "\n",
        "# test for three-gram\n",
        "print(model[('R', 'T')][' ']) \n",
        "print(modelF[('R', 'T')][' '])\n",
        "\n",
        "# test for 4-gram\n",
        "# print(model[('R', 'T', ' ')][' '])\n",
        "# print(modelF[('R', 'T', ' ')][' '])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7005524861878453\n",
            "0.9788219722038385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R58V6aKiq6BZ"
      },
      "source": [
        "def eval(n, model, data_file):\n",
        "  prob = []\n",
        "  data = pd.read_csv(data_file)\n",
        "  \n",
        "  for i in data.index:\n",
        "    tweet = data['tweet_text'][i]    \n",
        "    k = len(tweet)\n",
        "\n",
        "    for j in range(0,k-n):\n",
        "      n_minus_1_gram = tuple(list(tweet[j:j+n-1]))\n",
        "      n_th_char = tweet[j+n-1]\n",
        "\n",
        "      if n == 1:\n",
        "        p_value = model[(n_th_char,)]\n",
        "      else:  \n",
        "        p_value = model[n_minus_1_gram][n_th_char]\n",
        "      \n",
        "      # According to instructor note in piazza, ignore p_value = 0, will change to p = 1 so that log(p) = 0\n",
        "      if p_value == 0:\n",
        "        p_value = 1\n",
        "\n",
        "      prob.append(p_value)\n",
        "\n",
        "  prob = np.array(prob)\n",
        "  h_x = -1*(np.log2(prob).mean())\n",
        "\n",
        "  return 2**h_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB-sOwE0aCxu"
      },
      "source": [
        "As we can see, the preplexity decrease when the n increase without add-one smoothing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww7dlk2va9La",
        "outputId": "5b63a492-68d2-4eda-aad9-7df6ed66e7ed"
      },
      "source": [
        "for i in range(1,5):\n",
        "  model_to_eval = lm(i, vocab, 'nlp-course/lm-languages-data-new/en.csv', False)\n",
        "  perp = eval(i,model_to_eval, 'nlp-course/lm-languages-data-new/en.csv')\n",
        "  print(f'Perplexity of model based on en.csv with n={i}: {perp}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of model based on en.csv with n=1: 36.37175674135632\n",
            "Perplexity of model based on en.csv with n=2: 17.812446482947372\n",
            "Perplexity of model based on en.csv with n=3: 8.78191067907144\n",
            "Perplexity of model based on en.csv with n=4: 4.473087603253531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn"
      },
      "source": [
        "def match(n, add_one):\n",
        "  # n - the n-gram to use for creating n-gram models\n",
        "  # add_one - use add_one smoothing or not\n",
        "  \n",
        "  df = pd.DataFrame(columns =  ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl'], index = ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl'])\n",
        "  vocab = preprocess()\n",
        "  for ind in df.index:\n",
        "    file_path_lang = 'nlp-course/lm-languages-data-new/' + ind + '.csv'\n",
        "    model = lm(n, vocab, file_path_lang, add_one)\n",
        "    for col in df.columns:\n",
        "      file_path = 'nlp-course/lm-languages-data-new/' + col + '.csv'\n",
        "      df[col][ind] = eval(n, model, file_path)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee58e261-37a3-4f56-a07e-34fec405afa6"
      },
      "source": [
        "for i in range(1,5):\n",
        "  print(f'\\nEvaluation table for {i}-gram model without add-one smoothing\\n')\n",
        "  print(match(i, False))\n",
        "  print(f'\\nEvaluation table for {i}-gram model with add-one smoothing\\n')\n",
        "  print(match(i, True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation table for 1-gram model without add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  36.3718  36.1548  37.3242  38.4186   36.263   37.091  36.0055  41.5854\n",
            "es  39.1031  34.0706  36.7792  40.3493  36.3388  38.5385  33.7447  43.8624\n",
            "fr  38.5986  36.8581  35.5018  41.2117  37.0243  38.1238  35.7268  45.8036\n",
            "in  39.6237  35.7185  42.5719   35.199  39.0492  39.0412  36.7088  39.6835\n",
            "it  38.3622  36.2891  36.9337  40.0934  35.4928   38.239  35.1112  43.0466\n",
            "nl  37.7169  37.3643   38.983  38.3525  37.7293  35.3959  37.4361  43.0685\n",
            "pt  39.3343  34.9173  37.1023  39.6317    36.55  38.5397  34.7806  43.6094\n",
            "tl  39.0536  39.7435  42.8051  36.1025  38.5084  39.9539  40.7483   38.383\n",
            "\n",
            "Evaluation table for 1-gram model with add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  36.4241  38.5553  40.5718  39.1659  38.3813  37.4964  40.7872  42.2584\n",
            "es  39.7203  34.1266  38.7646  41.2297  37.8494  39.1956  37.3156  44.6898\n",
            "fr  39.3056  37.4551  35.5516   42.018  37.6586  38.6442  38.6671  46.5297\n",
            "in  40.1932  40.9751  44.6972  35.2553  40.9729  39.5675  43.0342  40.2602\n",
            "it  39.1634  36.7144  38.3906  41.0418  35.5563  38.8074  38.8129  43.8667\n",
            "nl  38.5025  38.4361  39.7247  39.2096     38.7  35.4531  40.3512  43.8544\n",
            "pt  40.0897  35.3059  38.4631  40.5276  38.2215  39.2506  34.8533  44.5276\n",
            "tl  39.8667  40.4553  45.2184  36.8826  40.4331  40.4908  42.5186  38.4542\n",
            "\n",
            "Evaluation table for 2-gram model without add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  17.8124  19.4487   19.596  24.3146  21.6523  22.7115  20.2599  23.6919\n",
            "es  25.6713  15.7709  20.6908  27.1011  19.1372  26.4778   18.311  26.8319\n",
            "fr  23.2361  18.2501  16.6968  26.4166  21.3603  24.1804  19.9146  27.5012\n",
            "in  23.9739   20.728  21.6472  17.5279  22.0006  24.0159  21.4215  21.2819\n",
            "it  25.6692  18.3957  21.3486  26.6756  16.1931  26.8694  18.6615  26.0476\n",
            "nl  22.3959  22.3901   22.841  24.8345  24.4414  17.2807  23.2788   25.526\n",
            "pt  25.7324  18.3454   21.425  27.7037  19.4757   27.118  15.9938  27.0483\n",
            "tl  22.1863  20.5596  21.4209  20.7866  21.0673  24.8461  20.6507  17.4213\n",
            "\n",
            "Evaluation table for 2-gram model with add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  20.6643  25.7349  26.8983  28.8142  26.2988    26.83  27.6552  28.5462\n",
            "es  31.4908  18.4068   27.206  33.3065  23.5565  32.1677  22.9389  33.5405\n",
            "fr  27.7986  24.1406  19.4047  31.9526  25.5586  28.7604  25.9105  34.0995\n",
            "in  29.1418  27.1313  32.2213  20.6553  27.7564   29.027  29.5316  26.0786\n",
            "it  31.0183  23.6523  28.0848  32.6593  18.9732  32.4209  24.6449  32.4218\n",
            "nl  26.6494  29.4157  29.7515   29.517  29.8488  19.9966  31.2152  31.2546\n",
            "pt   32.687  22.8002  28.3716  34.9226  24.8611  33.8513  19.3104  35.0241\n",
            "tl  26.8797  27.4693  31.9047  24.9081  26.5218  30.2073  29.9562  20.8637\n",
            "\n",
            "Evaluation table for 3-gram model without add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  8.78191  10.6877  8.97558   13.203  11.8311  10.3232  10.9187  11.8772\n",
            "es  11.3233  8.44308   9.3266  11.7185  9.97069  10.6839  8.94061  11.6669\n",
            "fr  11.0765  9.88406  8.40822  13.0381  11.2364   10.926  10.8851  12.3719\n",
            "in  11.0856  11.0164  9.86303  9.59954  11.3684  11.4156   11.002  10.0086\n",
            "it  11.3363  9.41791  9.63104  11.7289  8.38476  11.4717    9.449  10.9473\n",
            "nl   10.701  11.4617  9.68883  13.2153   12.726  8.86223  11.9204  11.8769\n",
            "pt  10.6975  8.58353  9.32405  11.2643  9.80821  10.9973  7.87443  10.4687\n",
            "tl  9.86839  10.5673  9.23411  10.9446  10.9798  10.7222  10.3865  8.37789\n",
            "\n",
            "Evaluation table for 3-gram model with add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  25.8691  46.9105  45.0438  71.9487  53.5313  55.9456  50.6195  65.7375\n",
            "es  67.1793   23.772  50.9978  83.6006  44.1227  76.2683  39.1697  82.8879\n",
            "fr   54.906  40.0734  23.9486  79.8453  50.3386  63.9338  47.8087  83.1473\n",
            "in  59.8211  54.1357  58.6449  29.7974  60.9976  66.9576  57.0322  54.2959\n",
            "it  65.8935  41.6038  51.8247  86.5102  24.4703  80.4321  43.0845  76.4124\n",
            "nl  50.9455  58.8312  56.8919  73.4641  67.3408   26.302  63.7478  72.8257\n",
            "pt  75.0185  42.1049  59.0261  95.5045  51.8191  89.6063  24.9584  87.6393\n",
            "tl  51.3865  55.0237  57.8309  55.1243  57.3076  71.3421  57.1149  28.5187\n",
            "\n",
            "Evaluation table for 4-gram model without add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  4.47309  4.67193  4.14048  3.71359  4.34846  3.83908  4.03333  3.85093\n",
            "es  4.03538  4.72108  3.87404  3.39136  4.49353  3.24533  4.07903  3.58052\n",
            "fr  4.34754  4.58249  4.46359  3.59825  4.56966  3.58548  4.25007  3.49604\n",
            "in  4.35997    4.513  3.91581   4.9938  4.39153  3.85501   4.0668  4.33922\n",
            "it  3.96526  4.68986  3.93567  3.33111  4.58267  3.18252  4.35115  3.51025\n",
            "nl   4.5648  4.16278  3.98674  3.82876  4.12949  4.52171  3.86019  3.73928\n",
            "pt  3.60941  4.09792  3.68497  3.03127  4.10019  2.99062  4.32619  3.26441\n",
            "tl  4.20968  4.61926  3.80063    4.487  4.51871    3.615  4.09261  4.25729\n",
            "\n",
            "Evaluation table for 4-gram model with add-one smoothing\n",
            "\n",
            "         en       es       fr       in       it       nl       pt       tl\n",
            "en  60.2001  120.162  82.6802  167.038  137.225  101.159  122.357  139.468\n",
            "es  132.711   55.053  85.2127  144.911  98.0401   109.65  74.2966  149.244\n",
            "fr  119.147  92.9861  53.2642  169.317  124.408  111.211   109.23  162.879\n",
            "in  133.305  139.689  112.273  76.1993  149.483  136.506   139.75  110.822\n",
            "it  131.051  84.7353  94.9729  141.765  56.9392  120.354  87.2188  133.721\n",
            "nl  114.364  133.346   98.366  170.489   159.27  60.7892  145.543  152.481\n",
            "pt  136.034  78.1173  96.9388  146.822  110.505  126.059  57.7044  142.263\n",
            "tl  107.426  131.937  104.951  129.137  140.057  128.629  128.808  67.4375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4h5Cl0q2nR"
      },
      "source": [
        "**Part 6**\n",
        "\n",
        "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
        "\n",
        "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be excepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy_qCuE7lBNu"
      },
      "source": [
        "Loading data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "E3Qz4RHslQ1P",
        "outputId": "f813aebc-a278-47cb-8f28-247b0d82a119"
      },
      "source": [
        "data = pd.read_csv('nlp-course/lm-languages-data-new/test.csv', index_col = 'tweet_id')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>845394879479996416</th>\n",
              "      <td>RT @jarsofshine: In 08 I had a volunteer who h...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836313846675619841</th>\n",
              "      <td>IN OGNI CASO CON LE PAGHE CHE GIRANO IN Africa...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836259442328940544</th>\n",
              "      <td>@jaynaldmase @acobasilianne @dingDANGdantes @d...</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847729104472358912</th>\n",
              "      <td>Daags voor @RondeVlaanderen, @VoltaClassic als...</td>\n",
              "      <td>nl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836491739699412992</th>\n",
              "      <td>RT @ertsul20: Susuportahan kita hanggang sa du...</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                           tweet_text label\n",
              "tweet_id                                                                   \n",
              "845394879479996416  RT @jarsofshine: In 08 I had a volunteer who h...    en\n",
              "836313846675619841  IN OGNI CASO CON LE PAGHE CHE GIRANO IN Africa...    it\n",
              "836259442328940544  @jaynaldmase @acobasilianne @dingDANGdantes @d...    tl\n",
              "847729104472358912  Daags voor @RondeVlaanderen, @VoltaClassic als...    nl\n",
              "836491739699412992  RT @ertsul20: Susuportahan kita hanggang sa du...    tl"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyAkknBNlpOI"
      },
      "source": [
        "Creating features: <br>\n",
        "We decided to use cross entropy of different models as features. For each language and n $ \\in $ {1,2,3,4} with add-on smoothing we calcualted  a language model. With these 32 models we calcualted entopry of a single tweet.  <br>\n",
        "In addition, we also added 8 features that were not based on the language models but were based on the unique word vocabulary or each language. For each language corpus, a unique set of words was created. This is all the words that appear in the language corpus itself and do not appear in any of the other corpuses. So for each language we have a set of unique words and for each tweet we calculate the percent of words in the tweet that appear in the unique set per language out of total words in the tweet. Words are defined as any set of characters that are joined together without interuption of a space char.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QajzcxrSup0k"
      },
      "source": [
        "# Creating all 32 add-on models\n",
        "def models(vocab):\n",
        "  models = {}\n",
        "  languages = ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "\n",
        "  for l in languages:\n",
        "    file_path_lang = 'nlp-course/lm-languages-data-new/' + l + '.csv'\n",
        "    for i in range(1,5):\n",
        "      add_one = True\n",
        "      key_true = str(i) + '_' + l + '_True'\n",
        "      model = lm(i, vocab, file_path_lang, add_one)\n",
        "      model['name'] = key_true\n",
        "      models[key_true] = model\n",
        "\n",
        "  return models  \n",
        "\n",
        "# Calculating entopy features\n",
        "def eval_tweet(tweet, model):\n",
        "  n = int(model['name'][0])\n",
        "  tweet = 'א'*(n-1) + tweet + 'ת'\n",
        "  prob = []\n",
        "  k = len(tweet)\n",
        "\n",
        "  for j in range(0,k-n):\n",
        "    n_minus_1_gram = tuple(list(tweet[j:j+n-1]))\n",
        "    n_th_char = tweet[j+n-1]\n",
        "\n",
        "    if n == 1:\n",
        "      p_value = model[(n_th_char,)]\n",
        "    else:  \n",
        "      p_value = model[n_minus_1_gram][n_th_char]\n",
        "\n",
        "    # According to instructor note in piazza, ignore p_value = 0, will change to p = 1 so that log(p) = 0\n",
        "    if p_value == 0:\n",
        "      p_value = 1\n",
        "\n",
        "    prob.append(p_value)\n",
        "\n",
        "  prob = np.array(prob)\n",
        "  prob = np.ma.masked_equal(prob,0)\n",
        "  h_x = -1*(np.log2(prob).mean())\n",
        "  return h_x  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0HHoYuWwPAN"
      },
      "source": [
        "models = models(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG2eZuryINlr"
      },
      "source": [
        "# Creating set of unique words per language\n",
        "def create_vocab(data_file):\n",
        "  id = pd.read_csv(data_file)\n",
        "  current_set = set()\n",
        "  for i in id.index:\n",
        "      add = set(id.iloc[i,1].split())\n",
        "      current_set =  current_set | add \n",
        "  return current_set\n",
        "\n",
        "def find_unique_word_in_language(id):\n",
        "  languages = ['en' ,'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "  path = 'nlp-course/lm-languages-data-new/' + id + '.csv'\n",
        "  corpus_language = create_vocab(path)\n",
        "  diff_lang = set()\n",
        "  for lang in languages:\n",
        "    if lang != id:\n",
        "      path2 =  'nlp-course/lm-languages-data-new/' + lang + '.csv'\n",
        "      new = create_vocab(path2)\n",
        "      diff_lang = diff_lang | new\n",
        "  final = corpus_language - diff_lang  \n",
        "  return final\n",
        "\n",
        "# Calculates percent of unique words in tweet per language\n",
        "def count_unique(tweet, lang_set):\n",
        "  tweet = str(tweet).split()\n",
        "  word_length = len(tweet)\n",
        "  counter = 0\n",
        "  for word in tweet:\n",
        "    if word in lang_set:\n",
        "      counter += 1\n",
        "  return counter/word_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH_yxU2Kwm6k"
      },
      "source": [
        "es_unique = find_unique_word_in_language('es')\n",
        "en_unique = find_unique_word_in_language('en')\n",
        "fr_unique = find_unique_word_in_language('fr')\n",
        "in_unique = find_unique_word_in_language('in')\n",
        "it_unique = find_unique_word_in_language('it')\n",
        "nl_unique = find_unique_word_in_language('nl')\n",
        "pt_unique = find_unique_word_in_language('pt')\n",
        "tl_unique = find_unique_word_in_language('tl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls0N9PW4JOMk"
      },
      "source": [
        "sets = {'es' : es_unique, 'en' :en_unique, 'fr': fr_unique, 'in':in_unique, 'it':it_unique, 'nl':nl_unique, 'pt':pt_unique,'tl': tl_unique}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfSW1bDnIWuS"
      },
      "source": [
        "def add_features(data):\n",
        "  for key , value in models.items():\n",
        "    name = value['name']\n",
        "    n = int(value['name'][0])\n",
        "    data[f\"{name}\"]=data.apply(lambda data: eval_tweet(data['tweet_text'], value ),axis=1, result_type='expand')\n",
        "\n",
        "  for lang in sets:\n",
        "    lang_set = sets[lang]\n",
        "    data[f\"{lang}\"]=data.apply(lambda data: count_unique(data['tweet_text'], lang_set ),axis=1, result_type='expand')  \n",
        "    \n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rldtdStIhBt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "cd0c282b-29c1-4d63-8ff5-95b5d7ba4b24"
      },
      "source": [
        "# Creating features columns in dataset\n",
        "data = add_features(data)\n",
        "print('40 features added to dataset')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40 features added to dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>label</th>\n",
              "      <th>1_en_True</th>\n",
              "      <th>2_en_True</th>\n",
              "      <th>3_en_True</th>\n",
              "      <th>4_en_True</th>\n",
              "      <th>1_es_True</th>\n",
              "      <th>2_es_True</th>\n",
              "      <th>3_es_True</th>\n",
              "      <th>4_es_True</th>\n",
              "      <th>1_fr_True</th>\n",
              "      <th>2_fr_True</th>\n",
              "      <th>3_fr_True</th>\n",
              "      <th>4_fr_True</th>\n",
              "      <th>1_in_True</th>\n",
              "      <th>2_in_True</th>\n",
              "      <th>3_in_True</th>\n",
              "      <th>4_in_True</th>\n",
              "      <th>1_it_True</th>\n",
              "      <th>2_it_True</th>\n",
              "      <th>3_it_True</th>\n",
              "      <th>4_it_True</th>\n",
              "      <th>1_nl_True</th>\n",
              "      <th>2_nl_True</th>\n",
              "      <th>3_nl_True</th>\n",
              "      <th>4_nl_True</th>\n",
              "      <th>1_pt_True</th>\n",
              "      <th>2_pt_True</th>\n",
              "      <th>3_pt_True</th>\n",
              "      <th>4_pt_True</th>\n",
              "      <th>1_tl_True</th>\n",
              "      <th>2_tl_True</th>\n",
              "      <th>3_tl_True</th>\n",
              "      <th>4_tl_True</th>\n",
              "      <th>es</th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>in</th>\n",
              "      <th>it</th>\n",
              "      <th>nl</th>\n",
              "      <th>pt</th>\n",
              "      <th>tl</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>845394879479996416</th>\n",
              "      <td>RT @jarsofshine: In 08 I had a volunteer who h...</td>\n",
              "      <td>en</td>\n",
              "      <td>4.806124</td>\n",
              "      <td>4.128117</td>\n",
              "      <td>4.419504</td>\n",
              "      <td>5.949909</td>\n",
              "      <td>4.925381</td>\n",
              "      <td>4.659428</td>\n",
              "      <td>5.680545</td>\n",
              "      <td>7.773401</td>\n",
              "      <td>4.931331</td>\n",
              "      <td>4.696791</td>\n",
              "      <td>5.779017</td>\n",
              "      <td>7.919382</td>\n",
              "      <td>4.965101</td>\n",
              "      <td>4.608029</td>\n",
              "      <td>5.797560</td>\n",
              "      <td>7.831239</td>\n",
              "      <td>4.949518</td>\n",
              "      <td>4.755509</td>\n",
              "      <td>5.891226</td>\n",
              "      <td>8.001155</td>\n",
              "      <td>4.895818</td>\n",
              "      <td>4.601566</td>\n",
              "      <td>5.447773</td>\n",
              "      <td>7.640374</td>\n",
              "      <td>4.895037</td>\n",
              "      <td>4.734157</td>\n",
              "      <td>5.970497</td>\n",
              "      <td>7.784575</td>\n",
              "      <td>4.982964</td>\n",
              "      <td>4.547294</td>\n",
              "      <td>5.502566</td>\n",
              "      <td>7.346213</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836313846675619841</th>\n",
              "      <td>IN OGNI CASO CON LE PAGHE CHE GIRANO IN Africa...</td>\n",
              "      <td>it</td>\n",
              "      <td>5.646338</td>\n",
              "      <td>5.043846</td>\n",
              "      <td>6.444367</td>\n",
              "      <td>8.265513</td>\n",
              "      <td>5.669175</td>\n",
              "      <td>4.921034</td>\n",
              "      <td>6.167898</td>\n",
              "      <td>7.585042</td>\n",
              "      <td>5.810842</td>\n",
              "      <td>5.141068</td>\n",
              "      <td>6.597034</td>\n",
              "      <td>7.635056</td>\n",
              "      <td>5.835198</td>\n",
              "      <td>5.257088</td>\n",
              "      <td>6.879072</td>\n",
              "      <td>8.611460</td>\n",
              "      <td>5.546321</td>\n",
              "      <td>4.392376</td>\n",
              "      <td>4.979507</td>\n",
              "      <td>6.449854</td>\n",
              "      <td>5.832572</td>\n",
              "      <td>5.558514</td>\n",
              "      <td>7.248568</td>\n",
              "      <td>8.132850</td>\n",
              "      <td>5.659642</td>\n",
              "      <td>4.964194</td>\n",
              "      <td>6.494578</td>\n",
              "      <td>7.958456</td>\n",
              "      <td>5.615826</td>\n",
              "      <td>5.138169</td>\n",
              "      <td>6.474877</td>\n",
              "      <td>8.438085</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836259442328940544</th>\n",
              "      <td>@jaynaldmase @acobasilianne @dingDANGdantes @d...</td>\n",
              "      <td>tl</td>\n",
              "      <td>4.889307</td>\n",
              "      <td>4.291687</td>\n",
              "      <td>5.783332</td>\n",
              "      <td>8.052264</td>\n",
              "      <td>4.755791</td>\n",
              "      <td>4.349695</td>\n",
              "      <td>5.609305</td>\n",
              "      <td>7.708268</td>\n",
              "      <td>4.868811</td>\n",
              "      <td>4.444821</td>\n",
              "      <td>5.928465</td>\n",
              "      <td>8.050271</td>\n",
              "      <td>4.755671</td>\n",
              "      <td>4.190791</td>\n",
              "      <td>5.506231</td>\n",
              "      <td>7.674738</td>\n",
              "      <td>4.841224</td>\n",
              "      <td>4.428876</td>\n",
              "      <td>5.661525</td>\n",
              "      <td>7.939430</td>\n",
              "      <td>4.870653</td>\n",
              "      <td>4.502419</td>\n",
              "      <td>5.954939</td>\n",
              "      <td>7.853751</td>\n",
              "      <td>4.826941</td>\n",
              "      <td>4.390016</td>\n",
              "      <td>5.876185</td>\n",
              "      <td>8.035873</td>\n",
              "      <td>4.773894</td>\n",
              "      <td>4.127792</td>\n",
              "      <td>5.275387</td>\n",
              "      <td>7.656184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847729104472358912</th>\n",
              "      <td>Daags voor @RondeVlaanderen, @VoltaClassic als...</td>\n",
              "      <td>nl</td>\n",
              "      <td>5.224471</td>\n",
              "      <td>4.672308</td>\n",
              "      <td>5.723930</td>\n",
              "      <td>6.376911</td>\n",
              "      <td>5.224056</td>\n",
              "      <td>4.847826</td>\n",
              "      <td>6.094197</td>\n",
              "      <td>7.137180</td>\n",
              "      <td>5.247340</td>\n",
              "      <td>4.719948</td>\n",
              "      <td>5.874682</td>\n",
              "      <td>7.107212</td>\n",
              "      <td>5.352139</td>\n",
              "      <td>4.902325</td>\n",
              "      <td>6.394287</td>\n",
              "      <td>7.410632</td>\n",
              "      <td>5.201246</td>\n",
              "      <td>4.916412</td>\n",
              "      <td>6.080168</td>\n",
              "      <td>7.328718</td>\n",
              "      <td>5.184213</td>\n",
              "      <td>4.316553</td>\n",
              "      <td>4.925423</td>\n",
              "      <td>5.658421</td>\n",
              "      <td>5.243593</td>\n",
              "      <td>4.906557</td>\n",
              "      <td>6.191998</td>\n",
              "      <td>7.110749</td>\n",
              "      <td>5.341384</td>\n",
              "      <td>4.932243</td>\n",
              "      <td>6.213419</td>\n",
              "      <td>7.071552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836491739699412992</th>\n",
              "      <td>RT @ertsul20: Susuportahan kita hanggang sa du...</td>\n",
              "      <td>tl</td>\n",
              "      <td>5.314212</td>\n",
              "      <td>4.489866</td>\n",
              "      <td>5.454357</td>\n",
              "      <td>6.347071</td>\n",
              "      <td>5.429339</td>\n",
              "      <td>4.555982</td>\n",
              "      <td>5.424989</td>\n",
              "      <td>6.413107</td>\n",
              "      <td>5.436612</td>\n",
              "      <td>4.631663</td>\n",
              "      <td>5.649457</td>\n",
              "      <td>6.324208</td>\n",
              "      <td>5.309260</td>\n",
              "      <td>4.239137</td>\n",
              "      <td>4.839582</td>\n",
              "      <td>5.599610</td>\n",
              "      <td>5.363777</td>\n",
              "      <td>4.494646</td>\n",
              "      <td>5.533508</td>\n",
              "      <td>5.915919</td>\n",
              "      <td>5.350184</td>\n",
              "      <td>4.589027</td>\n",
              "      <td>5.566110</td>\n",
              "      <td>6.078988</td>\n",
              "      <td>5.434420</td>\n",
              "      <td>4.717950</td>\n",
              "      <td>5.672761</td>\n",
              "      <td>6.428938</td>\n",
              "      <td>5.283590</td>\n",
              "      <td>4.101586</td>\n",
              "      <td>4.449843</td>\n",
              "      <td>4.834560</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                           tweet_text  ...        tl\n",
              "tweet_id                                                               ...          \n",
              "845394879479996416  RT @jarsofshine: In 08 I had a volunteer who h...  ...  0.000000\n",
              "836313846675619841  IN OGNI CASO CON LE PAGHE CHE GIRANO IN Africa...  ...  0.000000\n",
              "836259442328940544  @jaynaldmase @acobasilianne @dingDANGdantes @d...  ...  0.000000\n",
              "847729104472358912  Daags voor @RondeVlaanderen, @VoltaClassic als...  ...  0.000000\n",
              "836491739699412992  RT @ertsul20: Susuportahan kita hanggang sa du...  ...  0.416667\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6uuGul4mC96"
      },
      "source": [
        "Function for classifying language of tweet: <br>\n",
        "Using the 40 features we used linear logistic regression and random forest models to predict the classification. The data for logistic regression was scaled before running the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV4_116Gc6iw"
      },
      "source": [
        "label = preprocessing.LabelEncoder()\n",
        "label.fit(data['label'])\n",
        "y=label.transform(data['label'])\n",
        "X = data.iloc[:,2:].values\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Scaling data for linear logistic regression\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "scaler_test = preprocessing.StandardScaler().fit(X_test)\n",
        "X_test_scaled = scaler_test.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RajFRX0dYuD"
      },
      "source": [
        "# Linear Logistic Regression Model\n",
        "classifiermodel = LogisticRegression(max_iter=1000,C=4)\n",
        "classifiermodel.fit(X_train_scaled, y_train)\n",
        "llr_predictions = classifiermodel.predict(X_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjQhNcdcnf_U"
      },
      "source": [
        "# Random Forest Model\n",
        "rfclassifiermodel = RandomForestClassifier(criterion='entropy',random_state=0,n_estimators=50)\n",
        "rfclassifiermodel.fit(X_train,y_train)\n",
        "rf_predictions = rfclassifiermodel.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ECmLd3rktZ"
      },
      "source": [
        "**Part 7**\n",
        "\n",
        "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOBO3YQls66r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a4d4d6-5a9d-4ffe-9022-d83dfe4ee1cc"
      },
      "source": [
        "f1_llr =f1_score(y_test, llr_predictions,average='weighted')\n",
        "print(f'F1 score for Linear Logistic Regression Model:{np.round(f1_llr, 4)}')\n",
        "f1_rf = f1_score(y_test, rf_predictions,average='weighted')\n",
        "print(f'F1 score for Random Forest Model:{np.round(f1_rf, 4)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score for Linear Logistic Regression Model:0.9225\n",
            "F1 score for Random Forest Model:0.9093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW"
      },
      "source": [
        "# **Good luck!**"
      ]
    }
  ]
}